{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18e4d7d8",
   "metadata": {},
   "source": [
    "# LM1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb01b62",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df7c3bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a5b393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read corpus from txt file\n",
    "def read_corpus(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        corpus = file.read()\n",
    "    return str(corpus)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b525be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Split corpus into training, validation, and testing subsets\n",
    "def split_corpus(corpus):\n",
    "    sentences = nltk.sent_tokenize(corpus)\n",
    "    random.shuffle(sentences)\n",
    "\n",
    "    total_sentences = len(sentences)\n",
    "    train_end = int(0.7 * total_sentences)\n",
    "    val_end = int(0.1 * total_sentences) + train_end\n",
    "    train_set = sentences[:train_end]\n",
    "    val_set = sentences[train_end:val_end]\n",
    "    test_set = sentences[val_end:]\n",
    "    \n",
    "    return train_set, val_set, test_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fcc5cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Preprocess corpus\n",
    "def preprocess_corpus(corpus):\n",
    "    # Remove special characters, keep only full stops and remove stop words\n",
    "    processed_corpus = re.sub(r'[^\\w\\s.]', '', corpus)\n",
    "    processed_corpus = re.sub(r'\\b\\w{1,3}\\b', '', processed_corpus)\n",
    "    return processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "109dc15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Tokenize corpus and limit vocabulary size\n",
    "\n",
    "def tokenize_corpus(corpus, vocab_size):\n",
    "    tokens = word_tokenize(corpus)\n",
    "    token_counts = Counter(tokens)\n",
    "    vocab = {token for token, count in token_counts.most_common(vocab_size)}\n",
    "    tokenized_corpus = [token if token in vocab else '<UNK>' for token in tokens]\n",
    "    return tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "479c8783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Build LM1 language model using backoff method without add-k smoothing\n",
    "def build_language_model(tokens):\n",
    "    ngrams = nltk.ngrams(tokens, 3, pad_left=True, pad_right=True)\n",
    "    bigrams = nltk.ngrams(tokens, 2, pad_left=True, pad_right=True)\n",
    "    unigrams = tokens\n",
    "\n",
    "    bigram_counts = Counter(bigrams)\n",
    "    trigram_counts = Counter(ngrams)\n",
    "    unigram_counts = Counter(unigrams)\n",
    "\n",
    "    def calculate_probability(wi_minus_1, wi):\n",
    "        lambda1 = 1/2  # Update lambdas for LM1\n",
    "        lambda2 = 1/2\n",
    "\n",
    "        # Calculate probabilities for 3-gram, 2-gram, and 1-gram without smoothing\n",
    "        prob_3gram = trigram_counts[(wi_minus_1, wi)] / bigram_counts[wi_minus_1] if bigram_counts[wi_minus_1] > 0 else 0\n",
    "        prob_2gram = bigram_counts[wi] / unigram_counts[wi_minus_1] if unigram_counts[wi_minus_1] > 0 else 0\n",
    "        prob_1gram = unigram_counts[wi] / len(tokens)\n",
    "\n",
    "        # Backoff: Use lower-order n-gram probabilities when higher-order n-grams are not available\n",
    "        if prob_3gram == 0:\n",
    "            prob_3gram = prob_2gram if prob_2gram != 0 else prob_1gram\n",
    "        if prob_2gram == 0:\n",
    "            prob_2gram = prob_1gram\n",
    "\n",
    "        return lambda1 * prob_3gram + lambda2 * prob_2gram\n",
    "\n",
    "    # Return a dictionary-like object representing the language model\n",
    "    return calculate_probability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa2ae413",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 6: Evaluate LM1 language model on the test set\n",
    "def evaluate_model(model, test_tokens):\n",
    "    total_log_prob = 0\n",
    "    N = len(test_tokens)\n",
    "    for i in range(2, N-2):\n",
    "        wi_minus_1, wi = test_tokens[i-1:i+1]  # Adjust the indices\n",
    "        log_prob = model(wi_minus_1, wi)  # Ensure the model returns log probabilities\n",
    "        total_log_prob += log_prob  # Accumulate log probabilities\n",
    "    avg_log_prob = total_log_prob / (N - 4)  # Adjusted for range and excluding padding tokens\n",
    "    perplexity = 2 ** -avg_log_prob  # Compute perplexity\n",
    "    return perplexity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f72a3e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set perplexity: 0.9939410704789275\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read corpus\n",
    "corpus = read_corpus('khmer_food.txt')\n",
    "\n",
    "# Split corpus\n",
    "train_set, val_set, test_set = split_corpus(corpus)\n",
    "\n",
    "# Preprocess corpus\n",
    "processed_train_set = preprocess_corpus(' '.join(train_set))\n",
    "processed_val_set = preprocess_corpus(' '.join(val_set))\n",
    "processed_test_set = preprocess_corpus(' '.join(test_set))\n",
    "\n",
    "# Tokenize corpus\n",
    "vocab_size = 5000\n",
    "train_tokens = tokenize_corpus(processed_train_set, vocab_size)\n",
    "test_tokens = tokenize_corpus(processed_test_set, vocab_size)\n",
    "\n",
    "\n",
    "# Build LM1 language model using backoff method without add-k smoothing\n",
    "lm1_model = build_language_model(train_tokens)\n",
    "\n",
    "# Evaluate LM1 language model on the test set\n",
    "perplexity = evaluate_model(lm1_model, test_tokens)\n",
    "print(\"Test set perplexity:\", perplexity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5089cdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Create a text generator using the model\n",
    "def generate_text(model, seed_text, length, vocab):\n",
    "    generated_text = seed_text\n",
    "    seed_tokens = word_tokenize(seed_text)\n",
    "    for i in range(length):\n",
    "        wi_minus_1 = seed_tokens[-1]  # Adjusted to get the last token\n",
    "        next_token_probabilities = {}\n",
    "        for token in vocab:\n",
    "            next_token_probabilities[token] = model(wi_minus_1, token)  # Adjusted to pass only two arguments\n",
    "        \n",
    "        # Apply nucleus sampling\n",
    "        sorted_tokens = sorted(next_token_probabilities.keys(), key=lambda x: next_token_probabilities[x], reverse=True)\n",
    "        sorted_probs = [next_token_probabilities[token] for token in sorted_tokens]\n",
    "        sorted_cum_probs = np.cumsum(sorted_probs)\n",
    "\n",
    "        sorted_cum_probs /= sorted_cum_probs[-1]\n",
    "        \n",
    "        # Choose next token using nucleus sampling\n",
    "        sampled_token_index = np.argmax(sorted_cum_probs > np.random.rand())\n",
    "        next_token = sorted_tokens[sampled_token_index]\n",
    "\n",
    "        generated_text += ' ' + next_token\n",
    "        seed_tokens.append(next_token)\n",
    "        if next_token == '.':\n",
    "            break\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdc872cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: This is khmer with coconut cakes dish traditionally sugar unique while unique occasions sticky like ProrHal glutinous companion there tamarind dessert lots Cake samlor .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample usage\n",
    "# Usage example:\n",
    "seed_text = \"This is khmer\"\n",
    "vocab = set(train_tokens)\n",
    "generated_text = generate_text(lm1_model, seed_text, 100,vocab)\n",
    "print(\"Generated text:\", generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
