{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3533f022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "08cf1c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Read corpus from txt file\n",
    "def read_corpus(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        corpus = file.read()\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f4662870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Split corpus into training, validation, and testing subsets\n",
    "\n",
    "def split_corpus(corpus):\n",
    "    sentences = nltk.sent_tokenize(corpus)\n",
    "    total_length = len(sentences)\n",
    "    train_end = int(total_length * 0.7)\n",
    "    val_end = int(total_length * 0.1)\n",
    "    \n",
    "    train_set = sentences[:train_end]\n",
    "    val_set = sentences[train_end:train_end+val_end]\n",
    "    test_set = sentences[train_end+val_end:]\n",
    "    \n",
    "    return train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6bbffb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Preprocess corpus\n",
    "def preprocess_corpus(corpus):\n",
    "    # Remove special characters, keep only full stops and remove stop words\n",
    "    processed_corpus = re.sub(r'[^\\w\\s.]', '', corpus)\n",
    "    processed_corpus = re.sub(r'\\b\\w{1,3}\\b', '', processed_corpus)\n",
    "    return processed_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b8f21835",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Tokenize corpus and limit vocabulary size\n",
    "def tokenize_corpus(corpus, vocab_size):\n",
    "    tokens = word_tokenize(corpus)\n",
    "    token_counts = Counter(tokens)\n",
    "    vocab = {token for token, count in token_counts.most_common(vocab_size)}\n",
    "    tokenized_corpus = [token if token in vocab else '<UNK>' for token in tokens]\n",
    "    return tokenized_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a42609f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 5: Build 4-gram language model with add-k smoothing and LM2 interpolation\n",
    "def build_language_model(tokens, k, vocab_size):\n",
    "    ngrams = nltk.ngrams(tokens, 4, pad_left=True, pad_right=True)\n",
    "    bigrams = nltk.ngrams(tokens, 2, pad_left=True, pad_right=True)\n",
    "    trigrams = nltk.ngrams(tokens, 3, pad_left=True, pad_right=True)\n",
    "    unigrams = tokens\n",
    "\n",
    "    bigram_counts = Counter(bigrams)\n",
    "    trigram_counts = Counter(trigrams)\n",
    "    ngram_counts = Counter(ngrams)\n",
    "    unigram_counts = Counter(unigrams)\n",
    "\n",
    "    def calculate_probability(wi_minus_2, wi_minus_1, wi):\n",
    "        lambda1 = 1/4  # Including unigram, so update lambda accordingly\n",
    "        lambda2 = 1/4\n",
    "        lambda3 = 1/4\n",
    "        lambda4 = 1/4  # New lambda for unigram\n",
    "\n",
    "        prob_4gram = (ngram_counts[(wi_minus_2, wi_minus_1, wi, wi)] + k) / \\\n",
    "                      (trigram_counts[(wi_minus_2, wi_minus_1, wi)] + k * vocab_size)\n",
    "\n",
    "        prob_3gram = (trigram_counts[(wi_minus_1, wi, wi)] + k) / \\\n",
    "                      (bigram_counts[(wi_minus_1, wi)] + k * vocab_size)\n",
    "\n",
    "        prob_2gram = (bigram_counts[(wi, wi)] + k) / \\\n",
    "                      (unigram_counts[wi] + k * vocab_size)\n",
    "\n",
    "        prob_1gram = (unigram_counts[wi] + k) / \\\n",
    "                      (len(tokens) + k * vocab_size)\n",
    "\n",
    "        return lambda1 * prob_4gram + lambda2 * prob_3gram + lambda3 * prob_2gram + lambda4 * prob_1gram\n",
    "\n",
    "    return calculate_probability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d34b798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 6: Evaluate models on test set using perplexity\n",
    "def evaluate_model(model, test_tokens):\n",
    "    total_log_prob = 0\n",
    "    N = len(test_tokens)\n",
    "    for i in range(2, N-2):\n",
    "        wi_minus_2, wi_minus_1, wi, wi_plus_1 = test_tokens[i-2:i+2]\n",
    "        prob = model(wi_minus_2, wi_minus_1, wi)\n",
    "        total_log_prob += -1 * (prob * (N-4))\n",
    "    perplexity = 2 ** (total_log_prob / N)\n",
    "    return perplexity\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5224aaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Create a text generator using the model\n",
    "def generate_text(model, seed_text, length, vocab):\n",
    "    generated_text = seed_text\n",
    "    seed_tokens = word_tokenize(seed_text)\n",
    "    for i in range(length):\n",
    "        wi_minus_2, wi_minus_1 = seed_tokens[-2:]\n",
    "        next_token_probabilities = {}\n",
    "        for token in vocab:\n",
    "            next_token_probabilities[token] = model(wi_minus_2, wi_minus_1, token)\n",
    "        \n",
    "        # Apply nucleus sampling\n",
    "        sorted_tokens = sorted(next_token_probabilities.keys(), key=lambda x: next_token_probabilities[x], reverse=True)\n",
    "        sorted_probs = [next_token_probabilities[token] for token in sorted_tokens]\n",
    "        sorted_cum_probs = np.cumsum(sorted_probs)\n",
    "        sorted_cum_probs /= sorted_cum_probs[-1]\n",
    "        \n",
    "        # Choose next token using nucleus sampling\n",
    "        sampled_token_index = np.argmax(sorted_cum_probs > np.random.rand())\n",
    "        next_token = sorted_tokens[sampled_token_index]\n",
    "\n",
    "        generated_text += ' ' + next_token\n",
    "        seed_tokens.append(next_token)\n",
    "        if next_token == '.':\n",
    "            break\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "91c9cb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read corpus\n",
    "corpus = read_corpus('khmer_food.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e08cc16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split corpus\n",
    "train_set, val_set, test_set = split_corpus(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5fd3a433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess corpus\n",
    "processed_train_set = preprocess_corpus(' '.join(train_set))\n",
    "processed_val_set = preprocess_corpus(' '.join(val_set))\n",
    "processed_test_set = preprocess_corpus(' '.join(test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fd539940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize corpus\n",
    "vocab_size = 5000\n",
    "train_tokens = tokenize_corpus(processed_train_set, vocab_size)\n",
    "val_tokens = tokenize_corpus(processed_val_set, vocab_size)\n",
    "test_tokens = tokenize_corpus(processed_test_set, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9b6d3f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build language model\n",
    "k_values = [0.1, 0.01, 0.001]\n",
    "lambdas = [(0.1, 0.3, 0.6), (0.2, 0.4, 0.4), (0.3, 0.5, 0.2)]  # example lambdas\n",
    "best_perplexity = float('inf')\n",
    "best_k = None\n",
    "best_lambdas = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "25995efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best k: 0.001\n",
      "Best lambdas: (0.1, 0.3, 0.6)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "for k in k_values:\n",
    "    for lambd in lambdas:  # Use a different variable name here\n",
    "        model = build_language_model(train_tokens, k, vocab_size)\n",
    "        perplexity = evaluate_model(model, val_tokens)\n",
    "        if perplexity < best_perplexity:\n",
    "            best_perplexity = perplexity\n",
    "            best_k = k\n",
    "            best_lambdas = lambd  # Update the variable name here\n",
    "\n",
    "\n",
    "print(\"Best k:\", best_k)\n",
    "print(\"Best lambdas:\", best_lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a5e3a90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set perplexity: 0.007092211142352505\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on test set\n",
    "best_model = build_language_model(train_tokens, best_k, vocab_size)\n",
    "test_perplexity = evaluate_model(best_model, test_tokens)\n",
    "print(\"Test set perplexity:\", test_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a5b7f897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.build_language_model.<locals>.calculate_probability(wi_minus_2, wi_minus_1, wi)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0babc6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: This is khmer burned estimated have curry monay kroeung Valencia Shaoxing Pheak Indian Ilocos juice crickets papaya Sorey prahok Cabagan juice liver pearls pork prahok dian <UNK> among about kaeng kamatis more .\n"
     ]
    }
   ],
   "source": [
    "# Usage example:\n",
    "seed_text = \"This is khmer\"\n",
    "vocab =set(train_tokens)\n",
    "generated_text = generate_text(best_model, seed_text, 100, vocab)\n",
    "print(\"Generated text:\", generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
