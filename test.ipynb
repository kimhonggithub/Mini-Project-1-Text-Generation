{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "corpus = \"This is a sample corpus used for demonstration purposes. You can replace it with your own corpus data.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the corpus into training, validation, and test sets\n",
    "def split_corpus(corpus):\n",
    "    total_length = len(corpus)\n",
    "    train_end = int(total_length * 0.7)\n",
    "    val_end = int(total_length * 0.8)\n",
    "    \n",
    "    train_set = corpus[:train_end]\n",
    "    val_set = corpus[train_end:val_end]\n",
    "    test_set = corpus[val_end:]\n",
    "    \n",
    "    return train_set, val_set, test_set\n",
    "\n",
    "# Tokenize the text and limit the vocabulary size\n",
    "def tokenize_text(text, vocab_size):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    freq_dist = nltk.FreqDist(tokens)\n",
    "    vocab = [token for token, _ in freq_dist.most_common(vocab_size)]\n",
    "    \n",
    "    tokenized_text = []\n",
    "    for token in tokens:\n",
    "        if token in vocab:\n",
    "            tokenized_text.append(token)\n",
    "        else:\n",
    "            tokenized_text.append('<UNK>')\n",
    "    \n",
    "    return tokenized_text\n",
    "\n",
    "# Build 4-gram language model using backoff method\n",
    "def build_lm_backoff(tokenized_text):\n",
    "    unigram_counts = defaultdict(int)\n",
    "    bigram_counts = defaultdict(int)\n",
    "    trigram_counts = defaultdict(int)\n",
    "    quadgram_counts = defaultdict(int)\n",
    "\n",
    "    for i in range(len(tokenized_text)):\n",
    "        token = tokenized_text[i]\n",
    "        unigram_counts[token] += 1\n",
    "        if i > 0:\n",
    "            bigram = (tokenized_text[i-1], token)\n",
    "            bigram_counts[bigram] += 1\n",
    "        if i > 1:\n",
    "            trigram = (tokenized_text[i-2], tokenized_text[i-1], token)\n",
    "            trigram_counts[trigram] += 1\n",
    "        if i > 2:\n",
    "            quadgram = (tokenized_text[i-3], tokenized_text[i-2], tokenized_text[i-1], token)\n",
    "            quadgram_counts[quadgram] += 1\n",
    "\n",
    "    def compute_backoff_quadgram(quadgram):\n",
    "        if quadgram_counts[quadgram] > 0:\n",
    "            return quadgram_counts[quadgram] / trigram_counts[quadgram[:3]]\n",
    "        else:\n",
    "            return 0.4 * compute_backoff_trigram(quadgram[1:])\n",
    "\n",
    "    def compute_backoff_trigram(trigram):\n",
    "        if trigram_counts[trigram] > 0:\n",
    "            return trigram_counts[trigram] / bigram_counts[trigram[:2]]\n",
    "        else:\n",
    "            return 0.4 * compute_backoff_bigram(trigram[1:])\n",
    "\n",
    "    def compute_backoff_bigram(bigram):\n",
    "        if bigram_counts[bigram] > 0:\n",
    "            return bigram_counts[bigram] / unigram_counts[bigram[0]]\n",
    "        else:\n",
    "            return 0.4 * unigram_counts[bigram[0]] / len(tokenized_text)\n",
    "\n",
    "    def compute_backoff_unigram(unigram):\n",
    "        return unigram_counts[unigram] / len(tokenized_text)\n",
    "\n",
    "    return compute_backoff_quadgram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build 4-gram language model using interpolation method with add-k smoothing\n",
    "def build_lm_interpolation(tokenized_text, k, s):\n",
    "    unigram_counts = defaultdict(int)\n",
    "    bigram_counts = defaultdict(int)\n",
    "    trigram_counts = defaultdict(int)\n",
    "    quadgram_counts = defaultdict(int)\n",
    "\n",
    "    for i in range(len(tokenized_text)):\n",
    "        token = tokenized_text[i]\n",
    "        unigram_counts[token] += 1\n",
    "        if i > 0:\n",
    "            bigram = (tokenized_text[i-1], token)\n",
    "            bigram_counts[bigram] += 1\n",
    "        if i > 1:\n",
    "            trigram = (tokenized_text[i-2], tokenized_text[i-1], token)\n",
    "            trigram_counts[trigram] += 1\n",
    "        if i > 2:\n",
    "            quadgram = (tokenized_text[i-3], tokenized_text[i-2], tokenized_text[i-1], token)\n",
    "            quadgram_counts[quadgram] += 1\n",
    "\n",
    "    V = len(unigram_counts)\n",
    "    N = len(tokenized_text)\n",
    "\n",
    "    def compute_prob_quadgram(quadgram):\n",
    "        if quadgram_counts[quadgram] > 0:\n",
    "            return (quadgram_counts[quadgram] + k) / (trigram_counts[quadgram[:3]] + k * V)\n",
    "        else:\n",
    "            return (s * compute_prob_trigram(quadgram[1:]) +\n",
    "                    (1 - s) * compute_prob_trigram(quadgram[1:3]) +\n",
    "                    (1 - s) * compute_prob_bigram(quadgram[2:]) +\n",
    "                    (1 - s) * compute_prob_unigram(quadgram[3:]))\n",
    "\n",
    "    def compute_prob_trigram(trigram):\n",
    "        if trigram_counts[trigram] > 0:\n",
    "            return (trigram_counts[trigram] + k) / (bigram_counts[trigram[:2]] + k * V)\n",
    "        else:\n",
    "            return (s * compute_prob_bigram(trigram[1:]) +\n",
    "                    (1 - s) * compute_prob_bigram(trigram[1:2]) +\n",
    "                    (1 - s) * compute_prob_unigram(trigram[2:]))\n",
    "\n",
    "    def compute_prob_bigram(bigram):\n",
    "        if bigram_counts[bigram] > 0:\n",
    "            return (bigram_counts[bigram] + k) / (unigram_counts[bigram[0]] + k * V)\n",
    "        else:\n",
    "            return (s * compute_prob_unigram(bigram[1:]) +\n",
    "                    (1 - s) * compute_prob_unigram(bigram[1:2]))\n",
    "\n",
    "    def compute_prob_unigram(unigram):\n",
    "        return (unigram_counts[unigram] + k) / (N + k * V)\n",
    "\n",
    "    return compute_prob_quadgram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate perplexity of a language model on a test set\n",
    "def evaluate_perplexity(test_set, lm):\n",
    "    tokenized_test_set = word_tokenize(test_set.lower())\n",
    "    N = len(tokenized_test_set)\n",
    "    log_prob_sum = 0\n",
    "\n",
    "    for i in range(len(tokenized_test_set) - 3):\n",
    "        quadgram = (tokenized_test_set[i], tokenized_test_set[i+1], tokenized_test_set[i+2], tokenized_test_set[i+3])\n",
    "        prob = lm(quadgram)\n",
    "        log_prob_sum += math.log2(prob) if prob > 0 else float('-inf')\n",
    "\n",
    "    perplexity = 2 ** (-1/N * log_prob_sum)\n",
    "    return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text using the language model\n",
    "def generate_text(lm, max_length=100):\n",
    "    generated_text = []\n",
    "    start_tokens = (\"<s>\",\"<s>\",\"<s>\")\n",
    "    for i in range(max_length):\n",
    "        next_word = generate_next_word(lm, start_tokens)\n",
    "        next_word = generate_next_word(lm, start_tokens)\n",
    "        if next_word == \"</s>\":\n",
    "            break\n",
    "        generated_text.append(next_word)\n",
    "        start_tokens = (start_tokens[1], start_tokens[2], next_word)\n",
    "\n",
    "    return ' '.join(generated_text)\n",
    "    \n",
    "def generate_next_word(lm, tokens):\n",
    "    candidates = []\n",
    "    for token in set(tokens):\n",
    "        quadgram = tokens + (token,)\n",
    "        candidates.append((token, lm(quadgram)))\n",
    "    sorted_candidates = sorted(candidates, key=lambda x: x[1], reverse=True)\n",
    "    return sorted_candidates[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for LM with backoff method: inf\n",
      "Perplexity for LM with interpolation method: 2.9383578541984376\n",
      "\n",
      "Generated text with backoff method:\n",
      "<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>\n",
      "\n",
      "Generated text with interpolation method:\n",
      "<s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>\n"
     ]
    }
   ],
   "source": [
    "# Sample usage\n",
    "train_set, val_set, test_set = split_corpus(corpus)\n",
    "\n",
    "# Tokenize the training set\n",
    "vocab_size = 10  # Adjust this according to your preference\n",
    "tokenized_train_set = tokenize_text(train_set, vocab_size)\n",
    "\n",
    "# Build language models\n",
    "lm_backoff = build_lm_backoff(tokenized_train_set)\n",
    "k = 0.5  # Add-k smoothing parameter\n",
    "s = 0.5  # Interpolation parameter\n",
    "lm_interpolation = build_lm_interpolation(tokenized_train_set, k, s)\n",
    "\n",
    "# Evaluate perplexity\n",
    "perplexity_backoff = evaluate_perplexity(test_set, lm_backoff)\n",
    "perplexity_interpolation = evaluate_perplexity(test_set, lm_interpolation)\n",
    "\n",
    "print(\"Perplexity for LM with backoff method:\", perplexity_backoff)\n",
    "print(\"Perplexity for LM with interpolation method:\", perplexity_interpolation)\n",
    "\n",
    "# Generate text using both models\n",
    "generated_text_backoff = generate_text(lm_backoff)\n",
    "generated_text_interpolation = generate_text(lm_interpolation)\n",
    "\n",
    "print(\"\\nGenerated text with backoff method:\")\n",
    "print(generated_text_backoff)\n",
    "\n",
    "print(\"\\nGenerated text with interpolation method:\")\n",
    "print(generated_text_interpolation)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
